#!/bin/bash

# set root repo relatively to a test dir
ROOT_REPO=${ROOT_REPO:-$(realpath ../../..)}
test_name=$(basename "$(pwd)")
source "${ROOT_REPO}/e2e-tests/vars.sh"

if oc get projects 2>/dev/null; then
	OPENSHIFT=4
fi

init_temp_dir() {
	rm -rf "$TEMP_DIR"
	mkdir -p "$TEMP_DIR"
}

create_namespace() {
	local namespace=$1

	if [[ -n $OPENSHIFT ]]; then
		set -o pipefail
		if [[ $OPERATOR_NS ]] && (oc get project "$OPERATOR_NS" -o json >/dev/null 2>&1 | jq -r '.metadata.name' >/dev/null 2>&1); then
			oc delete --grace-period=0 --force=true project "$namespace" && sleep 120 || :
		else
			oc delete project "$namespace" && sleep 40 || :
		fi
		wait_for_delete "project/$namespace" || :

		oc new-project "$namespace"
		oc project "$namespace"
		oc adm policy add-scc-to-user hostaccess -z default || :
	else
		kubectl delete namespace $namespace --ignore-not-found || :
		kubectl wait --for=delete namespace "$namespace" || :
		kubectl create namespace $namespace
	fi
}

wait_for_delete() {
	local res="$1"

	echo -n "waiting for $res to be deleted"
	set +o xtrace
	retry=0
	until (kubectl get $res || :) 2>&1 | grep NotFound; do
		sleep 1
		echo -n .
		let retry+=1
		if [ $retry -ge 120 ]; then
			kubectl_bin logs ${OPERATOR_NS:+-n $OPERATOR_NS} $(get_operator_pod)
			echo max retry count $retry reached. something went wrong with operator or kubernetes cluster
			exit 1
		fi
	done
}

deploy_operator() {
	destroy_operator

	if [[ $OPERATOR_NS ]]; then
		create_namespace "${OPERATOR_NS}"
	fi

	kubectl -n "${OPERATOR_NS:-$NAMESPACE}" apply --server-side --force-conflicts -f "${DEPLOY_DIR}/crd.yaml"

	if [ -n "$OPERATOR_NS" ]; then
		kubectl -n "${OPERATOR_NS:-$NAMESPACE}" apply -f "${DEPLOY_DIR}/cw-rbac.yaml"

		yq eval \
			"$(printf 'select(documentIndex==1).spec.template.spec.containers[0].image="%s"' "${IMAGE}")" \
			"${DEPLOY_DIR}/cw-operator.yaml" \
			| yq eval '(select(documentIndex==1).spec.template.spec.containers[] | select(.name=="manager").env[] | select(.name=="DISABLE_TELEMETRY").value) = "true"' \
			| yq eval '(select(documentIndex==1).spec.template.spec.containers[] | select(.name=="manager").env[] | select(.name=="LOG_LEVEL").value) = "DEBUG"' \
			| kubectl -n "${OPERATOR_NS:-$NAMESPACE}" apply -f -
	else
		kubectl -n "${OPERATOR_NS:-$NAMESPACE}" apply -f "${DEPLOY_DIR}/rbac.yaml"

		yq eval \
			"$(printf 'select(documentIndex==1).spec.template.spec.containers[0].image="%s"' "${IMAGE}")" \
			"${DEPLOY_DIR}/operator.yaml" \
			| yq eval '(select(documentIndex==1).spec.template.spec.containers[] | select(.name=="manager").env[] | select(.name=="DISABLE_TELEMETRY").value) = "true"' \
			| yq eval '(select(documentIndex==1).spec.template.spec.containers[] | select(.name=="manager").env[] | select(.name=="LOG_LEVEL").value) = "DEBUG"' \
			| kubectl -n "${OPERATOR_NS:-$NAMESPACE}" apply -f -
	fi
}

destroy_operator() {
	kubectl -n "${OPERATOR_NS:-$NAMESPACE}" delete deployment percona-server-mysql-operator --force --grace-period=0 || true
	if [[ $OPERATOR_NS ]]; then
		kubectl delete namespace $OPERATOR_NS --force --grace-period=0 || true
	fi
}

deploy_non_tls_cluster_secrets() {
	kubectl -n "${NAMESPACE}" apply -f "${TESTS_CONFIG_DIR}/secrets.yaml"
}

deploy_tls_cluster_secrets() {
	kubectl -n "${NAMESPACE}" apply -f "${TESTS_CONFIG_DIR}/ssl-secret.yaml"
}

deploy_client() {
	kubectl -n "${NAMESPACE}" apply -f "${TESTS_CONFIG_DIR}/client.yaml"
}

apply_s3_storage_secrets() {
	kubectl -n "${NAMESPACE}" apply -f "${TESTS_CONFIG_DIR}/minio-secret.yml"
	kubectl -n "${NAMESPACE}" apply -f "${TESTS_CONFIG_DIR}/cloud-secret.yml"
}

deploy_pmm_server() {
	helm uninstall -n "${NAMESPACE}" monitoring || :
	helm repo remove percona || :
	kubectl delete clusterrole monitoring --ignore-not-found
	kubectl delete clusterrolebinding monitoring --ignore-not-found
	helm repo add percona https://percona.github.io/percona-helm-charts/
	helm repo update

	if [[ -n $OPENSHIFT ]]; then
		platform=openshift
		oc create sa pmm-server -n "${NAMESPACE}" || :
		oc adm policy add-scc-to-user privileged -z pmm-server -n "${NAMESPACE}" || :

		if [[ $OPERATOR_NS ]]; then
			timeout 30 oc delete clusterrolebinding $(kubectl get clusterrolebinding | grep 'pmm-ps-operator-' | awk '{print $1}') || :
			oc create clusterrolebinding pmm-ps-operator-cluster-wide --clusterrole=percona-server-mysql-operator --serviceaccount=$NAMESPACE:pmm-server -n "$NAMESPACE"
			oc patch clusterrole/percona-server-mysql-operator --type json -p='[{"op":"add","path": "/rules/-","value":{"apiGroups":["security.openshift.io"],"resources":["securitycontextconstraints"],"verbs":["use"],"resourceNames":["privileged"]}}]' ${OPERATOR_NS:+-n $OPERATOR_NS} || :
		else
			oc create rolebinding pmm-ps-operator-namespace-only --role percona-server-mysql-operator --serviceaccount=$NAMESPACE:pmm-server -n "$NAMESPACE"
			oc patch role/percona-server-mysql-operator --type json -p='[{"op":"add","path": "/rules/-","value":{"apiGroups":["security.openshift.io"],"resources":["securitycontextconstraints"],"verbs":["use"],"resourceNames":["privileged"]}}]' -n "$NAMESPACE" || :
		fi
		local additional_params="--set platform=openshift --set supresshttp2=false --set serviceAccount.create=false --set serviceAccount.name=pmm-server"
	fi

	retry 10 120 helm install monitoring percona/pmm -n "${NAMESPACE}" \
		--set fullnameOverride=monitoring \
		--version ${PMM_SERVER_VERSION} \
		--set image.tag=${IMAGE_PMM_SERVER#*:} \
		--set image.repository=${IMAGE_PMM_SERVER%:*} \
		--set service.type=LoadBalancer \
		$additional_params \
		--force
}

get_pmm_server_token() {
	local key_name=$1

	if [[ -z $key_name ]]; then
		key_name="operator"
	fi

	local ADMIN_PASSWORD
	ADMIN_PASSWORD=$(kubectl -n "${NAMESPACE}" get secret pmm-secret -o jsonpath="{.data.PMM_ADMIN_PASSWORD}" | base64 --decode)

	if [[ -z $ADMIN_PASSWORD ]]; then
		echo "Error: ADMIN_PASSWORD is empty or not found!" >&2
		return 1
	fi

	local create_response create_status_code create_json_response
	create_response=$(curl --insecure -s -X POST -H 'Content-Type: application/json' -H 'Accept: application/json' \
		-d "{\"name\":\"${key_name}\", \"role\":\"Admin\", \"isDisabled\":false}" \
		--user "admin:${ADMIN_PASSWORD}" \
		"https://$(get_service_ip monitoring-service)/graph/api/serviceaccounts" \
		-w "\n%{http_code}")

	create_status_code=$(echo "$create_response" | tail -n1)
	create_json_response=$(echo "$create_response" | sed '$ d')

	if [[ $create_status_code -ne 201 ]]; then
		echo "Error: Failed to create PMM service account. HTTP Status: $create_status_code" >&2
		echo "Response: $create_json_response" >&2
		return 1
	fi

	local service_account_id
	service_account_id=$(echo "$create_json_response" | jq -r '.id')

	if [[ -z $service_account_id || $service_account_id == "null" ]]; then
		echo "Error: Failed to extract service account ID!" >&2
		return 1
	fi

	local token_response token_status_code token_json_response
	token_response=$(curl --insecure -s -X POST -H 'Content-Type: application/json' \
		-d "{\"name\":\"${key_name}\"}" \
		--user "admin:${ADMIN_PASSWORD}" \
		"https://$(get_service_ip monitoring-service)/graph/api/serviceaccounts/${service_account_id}/tokens" \
		-w "\n%{http_code}")

	token_status_code=$(echo "$token_response" | tail -n1)
	token_json_response=$(echo "$token_response" | sed '$ d')

	if [[ $token_status_code -ne 200 ]]; then
		echo "Error: Failed to create token. HTTP Status: $token_status_code" >&2
		echo "Response: $token_json_response" >&2
		return 1
	fi

	echo "$token_json_response" | jq -r '.key'
}

delete_pmm_server_token() {
	local key_name=$1

	if [[ -z $key_name ]]; then
		key_name="operator"
	fi

	local ADMIN_PASSWORD
	ADMIN_PASSWORD=$(kubectl -n "${NAMESPACE}" get secret pmm-secret -o jsonpath="{.data.PMM_ADMIN_PASSWORD}" | base64 --decode)

	if [[ -z $ADMIN_PASSWORD ]]; then
		echo "Error: ADMIN_PASSWORD is empty or not found!" >&2
		return 1
	fi

	local user_credentials="admin:${ADMIN_PASSWORD}"

	local service_accounts_response service_accounts_status
	service_accounts_response=$(curl --insecure -s -X GET --user "${user_credentials}" \
		"https://$(get_service_ip monitoring-service)/graph/api/serviceaccounts/search" \
		-w "\n%{http_code}")

	service_accounts_status=$(echo "$service_accounts_response" | tail -n1)
	service_accounts_json=$(echo "$service_accounts_response" | sed '$ d')

	if [[ $service_accounts_status -ne 200 ]]; then
		echo "Error: Failed to fetch service accounts. HTTP Status: $service_accounts_status" >&2
		echo "Response: $service_accounts_json" >&2
		return 1
	fi

	local service_account_id
	service_account_id=$(echo "$service_accounts_json" | jq -r ".serviceAccounts[] | select(.name == \"${key_name}\").id")

	if [[ -z $service_account_id || $service_account_id == "null" ]]; then
		echo "Service account '${key_name}' not found."
		return 1
	fi

	local tokens_response tokens_status tokens_json
	tokens_response=$(curl --insecure -s -X GET --user "${user_credentials}" \
		"https://$(get_service_ip monitoring-service)/graph/api/serviceaccounts/${service_account_id}/tokens" \
		-w "\n%{http_code}")

	tokens_status=$(echo "$tokens_response" | tail -n1)
	tokens_json=$(echo "$tokens_response" | sed '$ d')

	if [[ $tokens_status -ne 200 ]]; then
		echo "Error: Failed to fetch tokens. HTTP Status: $tokens_status" >&2
		echo "Response: $tokens_json" >&2
		return 1
	fi

	local token_id
	token_id=$(echo "$tokens_json" | jq -r ".[] | select(.name == \"${key_name}\").id")

	if [[ -z $token_id || $token_id == "null" ]]; then
		echo "Token for service account '${key_name}' not found."
		return 1
	fi

	local delete_response delete_status
	delete_response=$(curl --insecure -s -X DELETE --user "${user_credentials}" \
		"https://$(get_service_ip monitoring-service)/graph/api/serviceaccounts/${service_account_id}/tokens/${token_id}" \
		-w "\n%{http_code}")

	delete_status=$(echo "$delete_response" | tail -n1)

	if [[ $delete_status -ne 200 ]]; then
		echo "Error: Failed to delete token. HTTP Status: $delete_status" >&2
		echo "Response: $delete_response" >&2
		return 1
	fi
}

deploy_minio() {
	local access_key
	local secret_key
	access_key="$(kubectl -n "${NAMESPACE}" get secret minio-secret -o jsonpath='{.data.AWS_ACCESS_KEY_ID}' | base64 -d)"
	secret_key="$(kubectl -n "${NAMESPACE}" get secret minio-secret -o jsonpath='{.data.AWS_SECRET_ACCESS_KEY}' | base64 -d)"

	helm uninstall -n "${NAMESPACE}" minio-service || :
	helm repo remove minio || :
	helm repo add minio https://charts.min.io/
	retry 10 60 helm install minio-service \
		-n "${NAMESPACE}" \
		--version "${MINIO_VER}" \
		--set replicas=1 \
		--set mode=standalone \
		--set resources.requests.memory=256Mi \
		--set rootUser=rootuser \
		--set rootPassword=rootpass123 \
		--set "users[0].accessKey"="$(printf '%q' "$(printf '%q' "$access_key")")" \
		--set "users[0].secretKey"="$(printf '%q' "$(printf '%q' "$secret_key")")" \
		--set "users[0].policy"=consoleAdmin \
		--set service.type=ClusterIP \
		--set configPathmc=/tmp/.minio/ \
		--set persistence.size=2G \
		--set securityContext.enabled=false \
		minio/minio
	MINIO_POD=$(kubectl -n "${NAMESPACE}" get pods --selector=release=minio-service -o 'jsonpath={.items[].metadata.name}')
	wait_pod $MINIO_POD

	# create bucket
	kubectl -n "${NAMESPACE}" run -i --rm aws-cli --image=perconalab/awscli --restart=Never -- \
		bash -c "AWS_ACCESS_KEY_ID='$access_key' AWS_SECRET_ACCESS_KEY='$secret_key' AWS_DEFAULT_REGION=us-east-1 \
        /usr/bin/aws --endpoint-url http://minio-service:9000 s3 mb s3://operator-testing"
}

retry() {
	local max=$1
	local delay=$2
	shift 2 # cut delay and max args
	local n=1

	until "$@"; do
		if [[ $n -ge $max ]]; then
			echo "The command ${*} has failed after $n attempts."
			exit 1
		fi
		((n++))
		sleep $delay
	done
}

get_operator_pod() {
	kubectl get pods -n "${OPERATOR_NS:-$NAMESPACE}" \
		--selector=app.kubernetes.io/name=percona-server-mysql-operator \
		-o 'jsonpath={.items[].metadata.name}'
}

get_cr() {
	local name_suffix=$1

	yq eval "$(printf '.metadata.name="%s"' "${test_name}${name_suffix:+-$name_suffix}")" "${DEPLOY_DIR}/cr.yaml" \
		| yq eval "$(printf '.spec.initImage="%s"' "${IMAGE}")" - \
		| yq eval '.spec.secretsName="test-secrets"' - \
		| yq eval '.spec.sslSecretName="test-ssl"' - \
		| yq eval '.spec.upgradeOptions.apply="disabled"' - \
		| yq eval '.spec.mysql.clusterType="async"' - \
		| yq eval '.spec.orchestrator.enabled=true' - \
		| yq eval "$(printf '.spec.mysql.image="%s"' "${IMAGE_MYSQL}")" - \
		| yq eval "$(printf '.spec.backup.image="%s"' "${IMAGE_BACKUP}")" - \
		| yq eval "$(printf '.spec.orchestrator.image="%s"' "${IMAGE_ORCHESTRATOR}")" - \
		| yq eval "$(printf '.spec.proxy.router.image="%s"' "${IMAGE_ROUTER}")" - \
		| yq eval "$(printf '.spec.toolkit.image="%s"' "${IMAGE_TOOLKIT}")" - \
		| yq eval "$(printf '.spec.proxy.haproxy.image="%s"' "${IMAGE_HAPROXY}")" - \
		| yq eval "$(printf '.spec.pmm.image="%s"' "${IMAGE_PMM_CLIENT}")" - \
		| if [ -n "${MINIKUBE}" ]; then
			yq eval '(.. | select(has("antiAffinityTopologyKey")).antiAffinityTopologyKey) |= "none"' - \
				| yq eval '.spec.proxy.haproxy.resources.requests.cpu="300m"' -
		else
			yq eval -
		fi
}

get_client_pod() {
	kubectl -n "${NAMESPACE}" get pods \
		--selector=name=mysql-client \
		-o 'jsonpath={.items[].metadata.name}'
}

run_mysql() {
	local command="$1"
	local uri="$2"
	local pod="$3"

	client_pod=$(get_client_pod)
	wait_pod $client_pod 1>&2

	kubectl -n "${NAMESPACE}" exec "${pod:-mysql-client}" -- \
		bash -c "printf '%s\n' \"${command}\" | mysql -sN $uri" 2>&1 \
		| sed -e 's/mysql: //' \
		| (grep -v 'Using a password on the command line interface can be insecure.' || :)
}

run_mysqlsh() {
	local command="$1"
	local uri="$2"
	local pod="$3"

	client_pod=$(get_client_pod)
	wait_pod $client_pod 1>&2

	kubectl -n "${NAMESPACE}" exec "${pod:-mysql-client}" -- \
		bash -c "printf '%s\n' \"${command}\" | mysqlsh --sql --quiet-start=2 $uri" 2>&1 \
		| tail -n +2
}

run_curl() {
	kubectl -n "${NAMESPACE}" exec mysql-client -- bash -c "curl -s -k $*"
}

get_innodb_cluster_name() {
	echo $(get_cluster_name) | tr -cd '[^a-zA-Z0-9_]+'
}

get_mysqlsh_uri() {
	local idx=${1:-0}

	echo "root:root_password@$(get_cluster_name)-mysql-${idx}.$(get_cluster_name)-mysql.${NAMESPACE}"
}

get_gr_status() {
	local uri="$1"
	local pod="$2"

	client_pod=$(get_client_pod)

	kubectl -n "${NAMESPACE}" exec "${pod:-mysql-client}" -- mysqlsh --uri $uri --cluster --result-format json -- cluster status \
		| sed -e 's/mysql: //' \
		| (grep -v 'Using a password on the command line interface can be insecure.' || :)
}

get_cluster_name() {
	kubectl -n "${NAMESPACE}" get ps -o jsonpath='{.items[0].metadata.name}'
}

get_mysql_service() {
	local cluster=$1

	echo "${cluster}-mysql"
}

get_router_service() {
	local cluster=$1

	echo "${cluster}-router"
}

get_haproxy_svc() {
	local cluster=$1

	echo "${cluster}-haproxy"
}

get_orc_svc() {
	local cluster=$1

	echo "${cluster}-orc"
}

get_mysql_headless_fqdn() {
	local cluster=$1
	local index=$2

	echo "${cluster}-mysql-${index}.${cluster}-mysql"
}

get_orc_headless_fqdn() {
	local cluster=$1
	local index=$2

	echo "${cluster}-orc-${index}.${cluster}-orc"
}

get_metric_values() {
	local metric=$1
	local instance=$2
	local token=$3
	local start=$($date -u "+%s" -d "-1 minute")
	local end=$($date -u "+%s")

	set +o xtrace
	retry=0
	until run_curl "-H 'Authorization: Bearer ${token}'" "https://monitoring-service/graph/api/datasources/proxy/1/api/v1/query_range?query=min%28$metric%7Bnode_name%3D%7E%22$instance%22%7d%20or%20$metric%7Bnode_name%3D%7E%22$instance%22%7D%29&start=$start&end=$end&step=60" | jq '.data.result[0].values[][1]' | grep '^"[0-9]*"$'; do
		sleep 1
		let retry+=1
		if [ $retry -ge 30 ]; then
			echo "Max retry count $retry reached. Data about instance $instance was not collected!"
			exit 1
		fi
	done
	set -o xtrace
}

get_qan20_values() {
	local instance=$1
	local token=$2
	local start=$($date -u "+%Y-%m-%dT%H:%M:%S" -d "-30 minute")
	local end=$($date -u "+%Y-%m-%dT%H:%M:%S")
	local endpoint=monitoring-service

	local payload=$(
		cat <<EOF
{
   "columns":[
      "load",
      "num_queries",
      "query_time"
   ],
   "first_seen": false,
   "group_by": "queryid",
   "include_only_fields": [],
   "keyword": "",
   "labels": [
       {
           "key": "cluster",
           "value": ["monitoring"]
   }],
   "limit": 10,
   "offset": 0,
   "order_by": "-load",
   "main_metric": "load",
   "period_start_from": "$($date -u -d '-12 hour' '+%Y-%m-%dT%H:%M:%S%:z')",
   "period_start_to": "$($date -u '+%Y-%m-%dT%H:%M:%S%:z')"
}
EOF
	)

	run_curl -XPOST -d "'$(echo ${payload} | sed 's/\n//g')'" "-H 'Authorization: Bearer ${token}'" "https://${user_pass}@${endpoint}/v1/qan/metrics:getReport" \
		| jq '.rows[].fingerprint'
}

get_mysql_pods() {
	kubectl get pod -n "${NAMESPACE}" --no-headers --selector=app.kubernetes.io/component=mysql | awk '{print $1}'
}

get_router_pods() {
	kubectl get pod -n "${NAMESPACE}" --no-headers --selector=app.kubernetes.io/component=router | awk '{print $1}'
}

get_mysql_users() {
	local args=$1

	run_mysql "SELECT user FROM mysql.user" "${args}" | grep -vE "mysql|root"
}

get_service_ip() {
	local service=$1

	while (kubectl get service/$service -n "${NAMESPACE}" -o 'jsonpath={.spec.type}' 2>&1 || :) | grep -q NotFound; do
		sleep 1
	done
	if [ "$(kubectl get service/$service -n "${NAMESPACE}" -o 'jsonpath={.spec.type}')" = "ClusterIP" ]; then
		kubectl get service/$service -n "${NAMESPACE}" -o 'jsonpath={.spec.clusterIP}'
		return
	fi
	until kubectl get service/$service -n "${NAMESPACE}" -o 'jsonpath={.status.loadBalancer.ingress[]}' 2>&1 | egrep -q "hostname|ip"; do
		sleep 1
	done
	kubectl get service/$service -n "${NAMESPACE}" -o 'jsonpath={.status.loadBalancer.ingress[].ip}'
	kubectl get service/$service -n "${NAMESPACE}" -o 'jsonpath={.status.loadBalancer.ingress[].hostname}'
}

wait_cluster_consistency_async() {
	local cluster_name=${1}
	local cluster_size=${2}
	local orc_size=${3}

	if [ -z "${orc_size}" ]; then
		orc_size=3
	fi

	sleep 7 # wait for two reconcile loops ;)  3 sec x 2 times + 1 sec = 7 seconds
	until [[ "$(kubectl get ps "${cluster_name}" -n "${NAMESPACE}" -o jsonpath='{.status.mysql.state}')" == "ready" &&
	"$(kubectl get ps "${cluster_name}" -n "${NAMESPACE}" -o jsonpath='{.status.mysql.ready}')" == "${cluster_size}" &&
	"$(kubectl get ps "${cluster_name}" -n "${NAMESPACE}" -o jsonpath='{.status.orchestrator.ready}')" == "${orc_size}" &&
	"$(kubectl get ps "${cluster_name}" -n "${NAMESPACE}" -o jsonpath='{.status.orchestrator.state}')" == "ready" &&
	"$(kubectl get ps "${cluster_name}" -n "${NAMESPACE}" -o jsonpath='{.status.state}')" == "ready" ]]; do
		echo 'waiting for cluster readyness (async)'
		sleep 15
	done
}

wait_cluster_consistency_gr() {
	local cluster_name=${1}
	local cluster_size=${2}
	local router_size=${3}

	if [ -z "${router_size}" ]; then
		router_size=3
	fi

	sleep 7 # wait for two reconcile loops ;)  3 sec x 2 times + 1 sec = 7 seconds
	until [[ "$(kubectl get ps "${cluster_name}" -n "${NAMESPACE}" -o jsonpath='{.status.mysql.state}')" == "ready" &&
	"$(kubectl get ps "${cluster_name}" -n "${NAMESPACE}" -o jsonpath='{.status.mysql.ready}')" == "${cluster_size}" &&
	"$(kubectl get ps "${cluster_name}" -n "${NAMESPACE}" -o jsonpath='{.status.router.ready}')" == "${router_size}" &&
	"$(kubectl get ps "${cluster_name}" -n "${NAMESPACE}" -o jsonpath='{.status.router.state}')" == "ready" &&
	"$(kubectl get ps "${cluster_name}" -n "${NAMESPACE}" -o jsonpath='{.status.state}')" == "ready" ]]; do
		echo 'waiting for cluster readyness (group replication)'
		sleep 15
	done
}

wait_pod() {
	local pod=$1

	set +o xtrace
	retry=0
	echo -n $pod
	until kubectl get pod/$pod -n "${NAMESPACE}" -o jsonpath='{.status.containerStatuses[0].ready}' 2>/dev/null | grep 'true'; do
		sleep 1
		echo -n .
		let retry+=1
		if [ $retry -ge 360 ]; then
			kubectl describe pod/$pod -n "${NAMESPACE}"
			kubectl logs $pod -n "${NAMESPACE}"
			kubectl logs $(get_operator_pod) -n "${OPERATOR_NS:-$NAMESPACE}" \
				| grep -v 'level=info' \
				| grep -v 'level=debug' \
				| grep -v 'Getting tasks for pod' \
				| grep -v 'Getting pods from source' \
				| tail -100
			echo max retry count $retry reached. something went wrong with operator or kubernetes cluster
			exit 1
		fi
	done
	set -o xtrace
}

wait_deployment() {
	local name=$1
	local target_namespace=${2:-"$namespace"}

	sleep 10
	set +o xtrace
	retry=0
	echo -n $name
	until [ -n "$(kubectl -n ${target_namespace} get deployment $name -o jsonpath='{.status.replicas}')" \
		-a "$(kubectl -n ${target_namespace} get deployment $name -o jsonpath='{.status.replicas}')" \
		== "$(kubectl -n ${target_namespace} get deployment $name -o jsonpath='{.status.readyReplicas}')" ]; do
		sleep 1
		echo -n .
		let retry+=1
		if [ $retry -ge 360 ]; then
			kubectl logs $(get_operator_pod) -c operator \
				| grep -v 'level=info' \
				| grep -v 'level=debug' \
				| tail -100
			echo max retry count $retry reached. something went wrong with operator or kubernetes cluster
			exit 1
		fi
	done
	echo
	set -o xtrace
}

check_auto_tuning() {
	local RAM_SIZE=$1
	local RDS_MEM_INSTANCE=12582880
	local CUSTOM_INNODB_SIZE=$2
	local CUSTOM_CONNECTIONS=$3

	local INNODB_SIZE=$(run_mysql \
		'SELECT @@innodb_buffer_pool_size;' \
		"-h $(get_haproxy_svc "$(get_cluster_name)") -uroot -proot_password")
	local CONNECTIONS=$(run_mysql \
		'SELECT @@max_connections;' \
		"-h $(get_haproxy_svc "$(get_cluster_name)") -uroot -proot_password")

	if [[ -n ${CUSTOM_INNODB_SIZE} ]]; then
		if [[ ${INNODB_SIZE} != ${CUSTOM_INNODB_SIZE} ]]; then
			echo "innodb_buffer_pool_size is set to ${INNODB_SIZE}, which does not correlate with ${CUSTOM_INNODB_SIZE} from custom config"
			exit 1
		fi
	else
		if [[ ${INNODB_SIZE} != $((RAM_SIZE * 50 / 100)) ]]; then
			echo "innodb_buffer_pool_size is set to ${INNODB_SIZE}, which does not correlate with cr.pxc.limits.memory * 0.5"
			exit 1
		fi
	fi

	if [[ -n ${CUSTOM_CONNECTIONS} ]]; then
		if [[ ${CONNECTIONS} != ${CUSTOM_CONNECTIONS} ]]; then
			echo "max_connections is set to ${AUTO_CONNECTIONS}, which does not correlate with ${CUSTOM_CONNECTIONS} from custom config"
			exit 1
		fi
	else
		if [[ ${CONNECTIONS} != $((RAM_SIZE / RDS_MEM_INSTANCE)) ]]; then
			echo "max_connections is set to ${CONNECTIONS}, which does not correlate with cr.pxc.limits.memory / ${RDS_MEM_INSTANCE}"
			exit 1
		fi
	fi
}

get_mysql_router_service() {
	local cluster=$1

	echo "${cluster}-router"
}

deploy_version_service() {
	kubectl create configmap -n "${OPERATOR_NS:-$NAMESPACE}" versions \
		--from-file "${TESTS_CONFIG_DIR}/operator.9.9.9.ps-operator.dep.json" \
		--from-file "${TESTS_CONFIG_DIR}/operator.9.9.9.ps-operator.json"

	kubectl apply -n "${OPERATOR_NS:-$NAMESPACE}" -f "${TESTS_CONFIG_DIR}/vs.yaml"

	sleep 5
}

deploy_cert_manager() {
	kubectl create namespace cert-manager || :
	kubectl label namespace cert-manager certmanager.k8s.io/disable-validation=true || :
	kubectl apply -f https://github.com/cert-manager/cert-manager/releases/download/v${CERT_MANAGER_VER}/cert-manager.yaml --validate=false || : 2>/dev/null
}

destroy_cert_manager() {
	kubectl delete -f https://github.com/cert-manager/cert-manager/releases/download/v${CERT_MANAGER_VER}/cert-manager.yaml --validate=false || : 2>/dev/null
	kubectl delete --grace-period=0 --force=true namespace cert-manager
}

get_primary_from_label() {
	kubectl -n "${NAMESPACE}" get pods -l mysql.percona.com/primary=true -ojsonpath="{.items[0].metadata.name}"
}

get_primary_from_haproxy() {
	local haproxy_pod=$1
	local haproxy_pod_ip=$(kubectl -n "${NAMESPACE}" get pods ${haproxy_pod} -o jsonpath="{.status.podIP}")

	run_mysql "SHOW VARIABLES LIKE '%hostname%';" "-h ${haproxy_pod_ip} -P3306 -uroot -proot_password" | awk '{print $2}'
}

get_primary_from_group_replication() {
	run_mysql \
		"SELECT MEMBER_HOST FROM performance_schema.replication_group_members where MEMBER_ROLE='PRIMARY';" \
		"-h $(get_mysql_router_service $(get_cluster_name)) -P 6446 -uroot -proot_password" \
		| cut -d'.' -f1
}

verify_certificate_sans() {
	local certificate=$1
	local expected_sans=$2
	local have=$(mktemp)
	local want=$(mktemp)

	kubectl -n "${NAMESPACE}" get certificate "${certificate}" -o jsonpath='{.spec.dnsNames}' | jq '.' >"${have}"
	echo "${expected_sans}" | jq '.' >"${want}"

	diff "${have}" "${want}"
}

check_passwords_leak() {
	local secrets
	local passwords
	local pods

	secrets=$(kubectl get secrets -o json | jq -r '.items[].data | to_entries | .[] | select(.key | (endswith(".crt") or endswith(".key") or endswith(".pub") or endswith(".pem") or endswith(".p12") or test("namespace")) | not) | .value')
	passwords="$(for i in $secrets; do
		base64 -d <<<$i
		echo
	done) $secrets"
	pods=$(kubectl -n "${NAMESPACE}" get pods -o name | awk -F "/" '{print $2}')

	collect_logs() {
		local containers
		local count

		NS=$1
		for p in $pods; do
			containers=$(kubectl -n "$NS" get pod $p -o jsonpath='{.spec.containers[*].name}')
			for c in $containers; do
				kubectl -n "$NS" logs $p -c $c >${TEMP_DIR}/logs_output-$p-$c.txt
				echo logs saved in: ${TEMP_DIR}/logs_output-$p-$c.txt
				for pass in $passwords; do
					count=$(grep -c --fixed-strings -- "$pass" ${TEMP_DIR}/logs_output-$p-$c.txt || :)
					count=$(echo "$count" | awk '{if ($1 ~ /^[0-9]+$/) print $1; else print 0}')
					if [[ $count != 0 ]]; then
						echo leaked passwords are found in log ${TEMP_DIR}/logs_output-$p-$c.txt
						false
					fi
				done
			done
			echo
		done
	}

	collect_logs $NAMESPACE
	if [ -n "$OPERATOR_NS" ]; then
		pods=$(kubectl -n "${OPERATOR_NS}" get pods -o name | awk -F "/" '{print $2}')
		collect_logs $OPERATOR_NS
	fi
}

deploy_chaos_mesh() {
	destroy_chaos_mesh

	helm repo add chaos-mesh https://charts.chaos-mesh.org
	if [ -n "${MINIKUBE}" ]; then
		helm install chaos-mesh chaos-mesh/chaos-mesh --namespace=${NAMESPACE} --set chaosDaemon.runtime=docker --set dashboard.create=false --version ${CHAOS_MESH_VER} --wait
	else
		helm install chaos-mesh chaos-mesh/chaos-mesh --namespace=${NAMESPACE} --set chaosDaemon.runtime=containerd --set chaosDaemon.socketPath=/run/containerd/containerd.sock --set dashboard.create=false --version ${CHAOS_MESH_VER}
	fi
	if [[ -n $OPENSHIFT ]]; then
		oc adm policy add-scc-to-user privileged -z chaos-daemon --namespace=${NAMESPACE}
	fi

	echo "Waiting for chaos-mesh DaemonSet to be ready..."
    until [ "$(kubectl get daemonset chaos-daemon -n ${NAMESPACE} -o jsonpath='{.status.numberReady}')" = "$(kubectl get daemonset chaos-daemon -n ${NAMESPACE} -o jsonpath='{.status.desiredNumberScheduled}')" ]; do
        echo "Waiting for DaemonSet chaos-daemon..."
        sleep 5
    done
}

destroy_chaos_mesh() {
	local chaos_mesh_ns=$(helm list --all-namespaces --filter chaos-mesh | tail -n1 | awk -F' ' '{print $2}' | sed 's/NAMESPACE//')

	if [ -n "${chaos_mesh_ns}" ]; then
		helm uninstall --wait --timeout 60s chaos-mesh --namespace ${chaos_mesh_ns} || :
	fi
	timeout 30 kubectl delete MutatingWebhookConfiguration $(kubectl get MutatingWebhookConfiguration | grep 'chaos-mesh' | awk '{print $1}') || :
	timeout 30 kubectl delete ValidatingWebhookConfiguration $(kubectl get ValidatingWebhookConfiguration | grep 'chaos-mesh' | awk '{print $1}') || :
	timeout 30 kubectl delete ValidatingWebhookConfiguration $(kubectl get ValidatingWebhookConfiguration | grep 'validate-auth' | awk '{print $1}') || :
	for i in $(kubectl api-resources | grep chaos-mesh | awk '{print $1}'); do
		kubectl get ${i} --all-namespaces --no-headers -o custom-columns=Kind:.kind,Name:.metadata.name,NAMESPACE:.metadata.namespace \
			| while read -r line; do
				local kind=$(echo "$line" | awk '{print $1}')
				local name=$(echo "$line" | awk '{print $2}')
				local namespace=$(echo "$line" | awk '{print $3}')
				kubectl patch $kind $name -n "${namespace}" --type=merge -p '{"metadata":{"finalizers":[]}}' || :
			done
		timeout 30 kubectl delete ${i} --all --all-namespaces || :
	done
	timeout 30 kubectl delete crd $(kubectl get crd | grep 'chaos-mesh.org' | awk '{print $1}') || :
	timeout 30 kubectl delete clusterrolebinding $(kubectl get clusterrolebinding | grep 'chaos-mesh' | awk '{print $1}') || :
	timeout 30 kubectl delete clusterrole $(kubectl get clusterrole | grep 'chaos-mesh' | awk '{print $1}') || :
}

kill_pods() {
	local ns=$1
	local selector=$2
	local pod_label=$3
	local label_value=$4
	local chaos_suffix=$5

	if [ "${selector}" == "pod" ]; then
		yq eval '
			.metadata.name = "chaos-pod-kill-'${chaos_suffix}'" |
			del(.spec.selector.pods.test-namespace) |
			.spec.selector.pods.'${ns}'[0] = "'${pod_label}'"' ${TESTS_CONFIG_DIR}/chaos-pod-kill.yml \
			| kubectl apply --namespace ${ns} -f -
	elif [ "${selector}" == "label" ]; then
		yq eval '
			.metadata.name = "chaos-kill-label-'${chaos_suffix}'" |
			.spec.mode = "all" |
			del(.spec.selector.pods) |
			.spec.selector.labelSelectors."'${pod_label}'" = "'${label_value}'"' ${TESTS_CONFIG_DIR}/chaos-pod-kill.yml \
			| kubectl apply --namespace ${ns} -f -
	fi
	sleep 5
}

failure_pod() {
	local ns=$1
	local pod=$2
	local chaos_suffix=$3

	yq eval '
        .metadata.name = "chaos-pod-failure-'${chaos_suffix}'" |
        del(.spec.selector.pods.test-namespace) |
        .spec.selector.pods.'${ns}'[0] = "'${pod}'"' ${TESTS_CONFIG_DIR}/chaos-pod-failure.yml \
		| kubectl apply --namespace ${ns} -f -
	sleep 5
}

network_loss() {
	local ns=$1
	local pod=$2
	local chaos_suffix=$3

	yq eval '
        .metadata.name = "chaos-pod-network-loss-'${chaos_suffix}'" |
        del(.spec.selector.pods.test-namespace) |
        .spec.selector.pods.'${ns}'[0] = "'${pod}'"' ${TESTS_CONFIG_DIR}/chaos-network-loss.yml \
		| kubectl apply --namespace ${ns} -f -
	sleep 5
}

renew_certificate() {
	certificate="$1"

	local pod_name
	pod_name=$(kubectl get -n "${NAMESPACE}" pods --selector=name=cmctl -o 'jsonpath={.items[].metadata.name}')

	local revision
	revision=$(kubectl get -n "${NAMESPACE}" certificate "$certificate" -o 'jsonpath={.status.revision}')

	kubectl exec -n "${NAMESPACE}" "$pod_name" -- /tmp/cmctl renew "$certificate"

	# wait for new revision
	for i in {1..10}; do
		local new_revision
		new_revision=$(kubectl get -n "${NAMESPACE}" certificate "$certificate" -o 'jsonpath={.status.revision}')
		if [ "$((revision + 1))" == "$new_revision" ]; then
			break
		fi
		sleep 1
	done
}

deploy_cmctl() {
	local service_account="cmctl"

	sed -e "s/percona-server-mysql-operator/$service_account/g" "${DEPLOY_DIR}/rbac.yaml" \
		| yq '(select(.rules).rules[] | select(contains({"apiGroups": ["cert-manager.io"]}))).resources += "certificates/status"' \
		| kubectl apply -n "${NAMESPACE}" -f -
	kubectl apply -n "${NAMESPACE}" -f "${TESTS_CONFIG_DIR}/cmctl.yml"
}
