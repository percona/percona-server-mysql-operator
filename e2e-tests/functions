#!/bin/bash

# set root repo relatively to a test dir
ROOT_REPO=${ROOT_REPO:-$(realpath ../../..)}
source "${ROOT_REPO}/e2e-tests/vars.sh"
test_name=$(basename "$(pwd)")

deploy_operator() {
	kubectl -n "${NAMESPACE}" apply -f "${DEPLOY_DIR}/crd.yaml"
	kubectl -n "${NAMESPACE}" apply -f "${DEPLOY_DIR}/rbac.yaml"

	yq eval \
		"$(printf 'select(documentIndex==1).spec.template.spec.containers[0].image="%s"' "${IMAGE}")" \
		"${DEPLOY_DIR}/operator.yaml" \
		| kubectl -n "${NAMESPACE}" apply -f -

	kubectl -n "${NAMESPACE}" apply -f "${TESTS_CONFIG_DIR}/secrets.yaml"
}

deploy_client() {
	kubectl -n "${NAMESPACE}" apply -f "${TESTS_CONFIG_DIR}/client.yaml"
}

deploy_pmm_server() {
	local platform=kubernetes
	if [ -n "${OPENSHIFT}" ]; then
		platform=openshift
		oc create sa pmm-server -n "${NAMESPACE}" || :
		oc adm policy add-scc-to-user privileged -z pmm-server -n "${NAMESPACE}" || :
		oc create rolebinding pmm-ps-operator-namespace-only --role percona-server-for-mysql-operator-role --serviceaccount=${NAMESPACE}:pmm-server -n "${NAMESPACE}" || :
		oc patch role/percona-server-for-mysql-operator-role --type json -p='[{"op":"add","path": "/rules/-","value":{"apiGroups":["security.openshift.io"],"resources":["securitycontextconstraints"],"verbs":["use"],"resourceNames":["privileged"]}}]' -n "${NAMESPACE}" || :
		helm install monitoring --set imageTag=$IMAGE_PMM_SERVER_TAG --set imageRepo=$IMAGE_PMM_SERVER_REPO --set platform=$platform --set sa=pmm-server --set supresshttp2=false https://percona-charts.storage.googleapis.com/pmm-server-${PMM_SERVER_VERSION}.tgz -n "${NAMESPACE}"
	else
		helm install monitoring \
			-n "${NAMESPACE}" \
			--set imageTag=$IMAGE_PMM_SERVER_TAG \
			--set imageRepo=$IMAGE_PMM_SERVER_REPO \
			--set platform="${platform}" \
			"https://percona-charts.storage.googleapis.com/pmm-server-${PMM_SERVER_VERSION}.tgz"
	fi
}

get_operator_pod() {
	kubectl get pods -n "${NAMESPACE}" \
		--selector=app.kubernetes.io/name=percona-server-mysql-operator \
		-o 'jsonpath={.items[].metadata.name}'
}

get_cr() {
	yq eval "$(printf '.metadata.name="%s"' "${test_name}")" "${DEPLOY_DIR}/cr.yaml" \
		| yq eval '.spec.secretsName="test-secrets"' - \
		| yq eval '.spec.sslSecretName="test-ssl"' - \
		| yq eval "$(printf '.spec.mysql.image="%s"' "${IMAGE_MYSQL}")" - \
		| yq eval "$(printf '.spec.orchestrator.image="%s"' "${IMAGE_ORCHESTRATOR}")" - \
		| yq eval "$(printf '.spec.pmm.image="%s"' "${IMAGE_MYSQL}")" -
}

run_mysql() {
	local command="$1"
	local uri="$2"
	local pod="$3"

	kubectl -n "${NAMESPACE}" exec "${pod:-mysql-client}" -- \
		bash -c "printf '%s\n' \"${command}\" | mysql -sN $uri" 2>&1 \
		| sed -e 's/mysql: //' \
		| (grep -v 'Using a password on the command line interface can be insecure.' || :)
}

run_curl() {
	kubectl -n "${NAMESPACE}" exec mysql-client -- bash -c "curl -s -k $*"
}

get_cluster_name() {
	kubectl -n "${NAMESPACE}" get ps -o jsonpath='{.items[0].metadata.name}'
}

get_mysql_primary_service() {
	local cluster=$1

	echo "${cluster}-mysql-primary"
}

get_mysql_headless_fqdn() {
	local cluster=$1
	local index=$2

	echo "${cluster}-mysql-${index}.${cluster}-mysql"
}

get_orc_headless_fqdn() {
	local cluster=$1
	local index=$2

	echo "${cluster}-orc-${index}.${cluster}-orc"
}

get_metric_values() {
	local metric=$1
	local instance=$2
	local user_pass=$3
	local start=$($date -u "+%s" -d "-1 minute")
	local end=$($date -u "+%s")

	run_curl "https://${user_pass}@monitoring-service/graph/api/datasources/proxy/1/api/v1/query_range?query=min%28$metric%7Bnode_name%3D%7E%22$instance%22%7d%20or%20$metric%7Bnode_name%3D%7E%22$instance%22%7D%29&start=$start&end=$end&step=60" \
		| jq '.data.result[0].values[][1]' \
		| grep '^"[0-9]'
}

get_qan20_values() {
	local instance=$1
	local user_pass=$2
	local start=$($date -u "+%Y-%m-%dT%H:%M:%S" -d "-30 minute")
	local end=$($date -u "+%Y-%m-%dT%H:%M:%S")
	local endpoint=monitoring-service

	local payload=$(
		cat <<EOF
{
   "columns":[
      "load",
      "num_queries",
      "query_time"
   ],
   "first_seen": false,
   "group_by": "queryid",
   "include_only_fields": [],
   "keyword": "",
   "labels": [
       {
           "key": "cluster",
           "value": ["monitoring"]
   }],
   "limit": 10,
   "offset": 0,
   "order_by": "-load",
   "main_metric": "load",
   "period_start_from": "$($date -u -d '-12 hour' '+%Y-%m-%dT%H:%M:%S%:z')",
   "period_start_to": "$($date -u '+%Y-%m-%dT%H:%M:%S%:z')"
}
EOF
	)

	run_curl -XPOST -d "'$(echo ${payload} | sed 's/\n//g')'" "https://${user_pass}@${endpoint}/v0/qan/GetReport" \
		| jq '.rows[].fingerprint'
}

get_mysql_pods() {
	kubectl get pod -n "${NAMESPACE}" --no-headers --selector=app.kubernetes.io/component=mysql | awk '{print $1}'
}

get_mysql_users() {
	local args=$1

	run_mysql "SELECT user FROM mysql.user" "${args}" | grep -vE "mysql|root"
}

get_service_ip() {
	local service=$1
	while (kubectl get service/$service -n "${NAMESPACE}" -o 'jsonpath={.spec.type}' 2>&1 || :) | grep -q NotFound; do
		sleep 1
	done
	if [ "$(kubectl get service/$service -n "${NAMESPACE}" -o 'jsonpath={.spec.type}')" = "ClusterIP" ]; then
		kubectl get service/$service -n "${NAMESPACE}" -o 'jsonpath={.spec.clusterIP}'
		return
	fi
	until kubectl get service/$service -n "${NAMESPACE}" -o 'jsonpath={.status.loadBalancer.ingress[]}' 2>&1 | egrep -q "hostname|ip"; do
		sleep 1
	done
	kubectl get service/$service -n "${NAMESPACE}" -o 'jsonpath={.status.loadBalancer.ingress[].ip}'
	kubectl get service/$service -n "${NAMESPACE}" -o 'jsonpath={.status.loadBalancer.ingress[].hostname}'
}

wait_cluster_consistency() {
	local cluster_name=${1}
	local cluster_size=${2}
	local orc_size=${3}

	if [ -z "${orc_size}" ]; then
		orc_size=1
	fi

	sleep 7 # wait for two reconcile loops ;)  3 sec x 2 times + 1 sec = 7 seconds
	until [[ "$(kubectl get ps "${cluster_name}" -n "${NAMESPACE}" -o jsonpath='{.status.mysql.state}')" == "ready" &&
	"$(kubectl get ps "${cluster_name}" -n "${NAMESPACE}" -o jsonpath='{.status.mysql.ready}')" == "${cluster_size}" &&
	"$(kubectl get ps "${cluster_name}" -n "${NAMESPACE}" -o jsonpath='{.status.orchestrator.state}')" == "ready" ]]; do
		echo 'waiting for cluster readyness'
		sleep 15
	done
}

wait_pod() {
	local pod=$1

	set +o xtrace
	retry=0
	echo -n $pod
	until kubectl get pod/$pod -n "${NAMESPACE}" -o jsonpath='{.status.containerStatuses[0].ready}' 2>/dev/null | grep 'true'; do
		sleep 1
		echo -n .
		let retry+=1
		if [ $retry -ge 360 ]; then
			kubectl describe pod/$pod -n "${NAMESPACE}"
			kubectl logs $pod -n "${NAMESPACE}"
			kubectl logs $(get_operator_pod) -n "${NAMESPACE}" \
				| grep -v 'level=info' \
				| grep -v 'level=debug' \
				| grep -v 'Getting tasks for pod' \
				| grep -v 'Getting pods from source' \
				| tail -100
			echo max retry count $retry reached. something went wrong with operator or kubernetes cluster
			exit 1
		fi
	done
	set -o xtrace
}
